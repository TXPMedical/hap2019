{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 対象者のデータセット確立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datetime as datetime\n",
    "from collections import Counter as ct\n",
    "import codecs\n",
    "from enum import IntEnum\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl as px\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "csv_path = '../data/'\n",
    "baseline = pd.read_csv(csv_path+'baseline.csv')\n",
    "\n",
    "e = baseline['ExclusionFlag']\n",
    "\n",
    "print (\n",
    "    \"Of \" + str(e.count()) + \"　patients transferred by ambulance to our center during the study period, we excluded \"  \\\n",
    "    + str((e==1).sum()) + \" patients of 6 years of age or younger, \"  \\\n",
    "    + str((e==2).sum()) + \" and patients with missing or invalid information on variables. \" \\\n",
    "    + \"A total of \" + str((e==0).sum()) + \" patients were included in the analysis.\"\n",
    ")\n",
    "\n",
    "encounter_list = baseline.drop(list(baseline.loc[baseline['ExclusionFlag'] != 0].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 既往歴の度数表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "icd_columns = [\n",
    "    'I10', 'E11', 'E14', 'I51', 'E78', 'I63', 'J45','I20', 'I48', 'F29', 'G30', 'K35', 'I50', 'I21', 'M48', 'K37', 'F03','K25', 'G40',\n",
    "    'C18', 'N18', 'H26', 'N19', 'F32', 'N40', 'K80', 'I49','E79', 'C16', 'R56'\n",
    "]\n",
    "\n",
    "freq_table_list = []\n",
    "for icd_code in icd_columns:\n",
    "    counter_dict = dict(ct(list(encounter_list[icd_code].values)))\n",
    "    true_freq = counter_dict[1]\n",
    "    false_freq = counter_dict[0]\n",
    "    true_prop = str(round(true_freq / (true_freq + false_freq)* 100, 2)) + '%'\n",
    "    false_prop = str(round(false_freq / (true_freq + false_freq) * 100, 2)) + '%'\n",
    "    elem_list = [icd_code, true_freq, false_freq, true_prop, false_prop]\n",
    "    freq_table_list.append(elem_list)\n",
    "    \n",
    "freq_table = pd.DataFrame(\n",
    "    np.array(freq_table_list), columns=['ICD_Code', 'True_freq', 'False_freq', 'True_prop', 'False_prop']\n",
    ")\n",
    "\n",
    "freq_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 予測性能の検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# モデルのインポート\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "\n",
    "class AnalysisTable:\n",
    "    def __init__(self, table):\n",
    "        self.table = table  # pd.DataFrame, XとYを結合したものを入れる\n",
    "        self.result_columns = [\n",
    "            'Target',\n",
    "            'Dataset',\n",
    "            'Fitting Model',\n",
    "            'Parameters',\n",
    "            'created at',\n",
    "            'TP, FP, TN, FN',\n",
    "            'Accuracy',\n",
    "            'Precision',\n",
    "            'Recall',\n",
    "            'Specificity',\n",
    "            'F-measure',\n",
    "            'PPV',\n",
    "            'NPV',\n",
    "            'ROC',\n",
    "            'AUC',\n",
    "            'AUCs',\n",
    "            'P.S.',\n",
    "        ]\n",
    "        self.result_table = pd.DataFrame(\n",
    "            columns=self.result_columns\n",
    "        )\n",
    "        self.models = {\n",
    "            'LogisticRegression': LogisticRegression(solver='lbfgs', max_iter=3000),\n",
    "            'DecisionTreeClassifier': DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=50),\n",
    "            'RandomForestClassifier': RandomForestClassifier(n_estimators=100),\n",
    "            'AdaBoostClassifier': AdaBoostClassifier(learning_rate=1.0),\n",
    "        }\n",
    "\n",
    "    def reset_result(self):\n",
    "        self.result_table = pd.DataFrame(columns=self.result_columns)\n",
    "\n",
    "    def select_params(self):\n",
    "        self.reset_result()\n",
    "        self.try_params()\n",
    "        \n",
    "    def try_params(self, column_indexes, target_column, use_model, cv=5, add_result_table=False, index_int=False, n_dups=0, dataset_name=''):\n",
    "        if index_int:\n",
    "            column_indexes = self.table.columns[column_indexes]\n",
    "        if dataset_name == '':\n",
    "            dataset_name = column_indexes\n",
    "        X = self.table[column_indexes].values\n",
    "        y = self.table[target_column].values\n",
    "        result = []\n",
    "        now = datetime.datetime.now()\n",
    "        model = self.models[use_model]\n",
    "        analysis_mean, report_mean, roc_curves, auc_cv_mean, auc_scores =\\\n",
    "            self.cross_validate_and_analyze(model, X, y, cv, n_dups=n_dups)\n",
    "        result.append([\n",
    "            target_column,  # Target\n",
    "            dataset_name,  # Dataset\n",
    "            use_model,  # Fitting Model\n",
    "            {'duplication': n_dups},  # Parameters\n",
    "            now,  # created at\n",
    "            analysis_mean,  # TP, FP, TN, FN\n",
    "            report_mean[0],  # Accuracy\n",
    "            report_mean[1],  # Precision\n",
    "            report_mean[2],  # Recall\n",
    "            report_mean[3],  # Specificity\n",
    "            report_mean[4],  # F-measure\n",
    "            report_mean[5],  # PPV\n",
    "            report_mean[6],  # NPV\n",
    "            roc_curves,  # ROC\n",
    "            auc_cv_mean, # mean AUC\n",
    "            auc_scores,  # AUC for CVs\n",
    "            '',  # P.S.\n",
    "        ])\n",
    "        result_df = pd.DataFrame(result, columns=self.result_columns)\n",
    "        self.result_table = pd.concat([self.result_table, result_df])\n",
    "\n",
    "     # cv: 交差検証の回数\n",
    "    def cross_validate_all_models(self, column_indexes, target_column,cv=5, n_dups=0):\n",
    "        X = self.table[column_indexes].values\n",
    "        y = self.table[target_column].values\n",
    "        result = []\n",
    "        now = datetime.datetime.now()\n",
    "        for model_name, model in self.models.items():\n",
    "            analysis_mean, report_mean, roc_curves, auc_cv_mean, auc_scores = \\\n",
    "                self.cross_validate_and_analyze(model, X, y, cv=cv, n_dups=n_dups)\n",
    "            result.append([\n",
    "                target_column,  # Target\n",
    "                column_indexes,  # Dataset\n",
    "                model_name,  # Fitting Model\n",
    "                {'duplication': n_dups},  # Parameters\n",
    "                now,  # created at\n",
    "                analysis_mean,  # TP, FP, TN, FN\n",
    "                report_mean[0],  # Accuracy\n",
    "                report_mean[1],  # Precision\n",
    "                report_mean[2],  # Recall\n",
    "                report_mean[3],  # Specificity\n",
    "                report_mean[4],  # F-measure\n",
    "                report_mean[5],  # PPV\n",
    "                report_mean[6],  # NPV\n",
    "                roc_curves,  # ROC\n",
    "                auc_cv_mean, # mean AUC\n",
    "                auc_scores,  # AUC for CVs\n",
    "                '',  # P.S.\n",
    "            ])\n",
    "        result_df = pd.DataFrame(result, columns=self.result_columns)\n",
    "        self.result_table = pd.concat([self.result_table, result_df])\n",
    "\n",
    "    def cross_validate_and_analyze(self, clf, X, y, cv, n_dups=0):\n",
    "        def perf_measure(y_actual, y_hat):\n",
    "            TP = 0\n",
    "            FP = 0\n",
    "            TN = 0\n",
    "            FN = 0\n",
    "            for i in range(len(y_hat)):\n",
    "                if y_actual[i] == y_hat[i] == 1:\n",
    "                    TP += 1\n",
    "                elif y_hat[i] == 1 and y_actual[i] == 0:\n",
    "                    FP += 1\n",
    "                elif y_actual[i] == y_hat[i] == 0:\n",
    "                    TN += 1\n",
    "                elif y_hat[i] == 0 and y_actual[i] == 1:\n",
    "                    FN += 1\n",
    "            return [TP, FP, TN, FN]\n",
    "\n",
    "        def calculate_report(result_array):\n",
    "                tp, fp, tn, fn = result_array\n",
    "                accuracy, precision, recall, specificity, f_measure, ppv, npv = 0, 0, 0, 0, 0, 0, 0\n",
    "                if (tp + fp + fn + tn) != 0:\n",
    "                    accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "                if (tp + fp) != 0:\n",
    "                    precision = tp / (tp + fp)\n",
    "                if (tp + fn) != 0:\n",
    "                    recall = tp / (tp + fn)\n",
    "                if (fp + tn) != 0:\n",
    "                    specificity = tn / (fp + tn)\n",
    "                if tp != 0:\n",
    "                    f_measure = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "                ppv = precision * 100\n",
    "\n",
    "                if (tn + fn) != 0:\n",
    "                    npv = tn * 100 / (tn + fn)\n",
    "\n",
    "                return [accuracy, precision, recall, specificity, f_measure, ppv, npv]\n",
    "\n",
    "        # 値が0のものをaxis毎に省いて平均値を計算する\n",
    "        def array_mean_except_zero(array, axis=0):\n",
    "            return np.nanmean(np.where(array != 0, array, np.nan), axis=axis)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=cv)\n",
    "        analysis = []\n",
    "        calculated_reports = []\n",
    "        fpr_tpr_test_probs = []\n",
    "        auc_scores = []\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            if n_dups != 0:\n",
    "                add_array = []\n",
    "                for _x, _y in zip(X_train, y_train):\n",
    "                    if _y == 1:\n",
    "                        add_array.append(_x)\n",
    "                for i in range(n_dups):\n",
    "                    X_train = np.r_[X_train, add_array]\n",
    "                    y_train = np.r_[y_train, np.ones(len(add_array))]\n",
    "            now = datetime.datetime.now()\n",
    "            clf = clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            measured_report = perf_measure(y_test, y_pred)\n",
    "            analysis.append(measured_report)\n",
    "            calculated = calculate_report(measured_report)\n",
    "            calculated_reports.append(calculated)\n",
    "\n",
    "            # ROC曲線とAUC\n",
    "            if type(clf) not in [LinearSVC, linear_model.LinearRegression, linear_model.Ridge]:\n",
    "                prob = clf.predict_proba(X_test)[:, 1]\n",
    "                fpr, tpr, thresholds = roc_curve(y_test, prob)\n",
    "                auc_score = roc_auc_score(y_test, prob)\n",
    "                fpr_tpr_test_probs.append([fpr, tpr, y_test, prob])\n",
    "                auc_scores.append(auc_score)\n",
    "\n",
    "        result = (\n",
    "            array_mean_except_zero(np.array(analysis)),\n",
    "            array_mean_except_zero(np.array(calculated_reports)),\n",
    "            fpr_tpr_test_probs,\n",
    "            np.mean(auc_scores),\n",
    "            auc_scores\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def reshape_result_table(self):\n",
    "        self.result_table = pd.DataFrame(columns=self.result_columns)\n",
    "\n",
    "\n",
    "# data_columnsの最後尾が disposition になるように作る\n",
    "def learning(encounter_list, data_columns, num_pred, cv=5, ps=''):\n",
    "    table = AnalysisTable(encounter_list.dropna(subset=data_columns)[data_columns])\n",
    "    table.cross_validate_all_models(column_indexes=data_columns[:-1], target_column=data_columns[-1], cv=cv)\n",
    "    table.result_table['P.S.'] = ps\n",
    "    table.result_table['Number of predictors'] = num_pred\n",
    "\n",
    "    result_columns = ['Dataset', 'Fitting Model', 'Number of predictors', 'AUC', 'AUCs','F-measure', 'Recall', 'Specificity', 'PPV', 'NPV', 'Accuracy', 'P.S.']\n",
    "    result = table.result_table[result_columns]\n",
    "    return result\n",
    "\n",
    "def learning_logistic(encounter_list, data_columns, dataset_name, num_pred, cv=5, ps=''):\n",
    "    table = AnalysisTable(encounter_list.dropna(subset=data_columns)[data_columns])\n",
    "    table.try_params(column_indexes=data_columns[:-1], target_column=data_columns[-1], use_model='LogisticRegression', \n",
    "                     cv=cv, dataset_name=dataset_name[:-1])\n",
    "\n",
    "    table.result_table['P.S.'] = ps\n",
    "    table.result_table['Number of predictors'] = num_pred\n",
    "    \n",
    "    result_columns = ['Dataset', 'Fitting Model', 'Number of predictors', 'AUC', 'AUCs','F-measure', 'Recall', 'Specificity', 'PPV', 'NPV', 'Accuracy', 'P.S.']\n",
    "    result = table.result_table[result_columns]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Cross Validation のためのデータ準備\n",
    "\n",
    "men_data = encounter_list.loc[encounter_list['PHR_BasicInfo::Sex'] == 1]  # データ数: 1832\n",
    "women_data = encounter_list.loc[encounter_list['PHR_BasicInfo::Sex'] == 0]  # データ数: 1377\n",
    "trauma_flag_1_data = encounter_list.loc[encounter_list['外傷flag'] == 1]  # データ数: 573\n",
    "trauma_flag_2_data = encounter_list.loc[encounter_list['外傷flag'] == 2]  # データ数: 2636\n",
    "senior_data = encounter_list.loc[encounter_list['Age'] >= 70]  # データ数: 1907\n",
    "young_data = encounter_list.loc[encounter_list['Age'] < 70]  # データ数: 1302\n",
    "\n",
    "\n",
    "def final_result(data_columns):\n",
    "    num_pred = len(data_columns)-1\n",
    "    \n",
    "    if 0:\n",
    "        result = pd.concat([learning(encounter_list, data_columns, num_pred, cv=5, ps='全体対象')])\n",
    "    else:\n",
    "        result0 = learning(encounter_list, data_columns, num_pred, cv=5, ps='全体対象')\n",
    "        result1 = learning(men_data, data_columns, num_pred, cv=4, ps='男性のみ対象')\n",
    "        result2 = learning(women_data, data_columns, num_pred, cv=4, ps='女性のみ対象')\n",
    "        result3 = learning(trauma_flag_1_data, data_columns, num_pred, cv=3, ps='「外傷flag = 1」のみ対象')\n",
    "        result4 = learning(trauma_flag_2_data, data_columns, num_pred, cv=4, ps='「外傷flag = 2」のみ対象')\n",
    "        result5 = learning(senior_data, data_columns, num_pred, cv=4, ps='70歳以上のみ対象')\n",
    "        result6 = learning(young_data, data_columns, num_pred, cv=4, ps='70歳未満のみ対象')\n",
    "        result = pd.concat([result0, result1, result2, result3, result4, result5, result6])\n",
    "    \n",
    "    return result\n",
    "\n",
    "#ロジスティック回帰のために年齢とJCSをダミー変数に置き換える\n",
    "def predictor_columns_for_logistic(column_indices):\n",
    "    ret = list()\n",
    "    for c in column_indices:\n",
    "        if c == 'DiscreteAge':\n",
    "            ret.append('Age0')\n",
    "            ret.append('Age1')\n",
    "            ret.append('Age2')\n",
    "            ret.append('Age3')\n",
    "            ret.append('Age4')\n",
    "            ret.append('Age6')\n",
    "            ret.append('Age7')\n",
    "            ret.append('Age8')\n",
    "            ret.append('Age9')\n",
    "        elif c== 'JCS':\n",
    "            ret.append('JCS1')\n",
    "            ret.append('JCS2')\n",
    "            ret.append('JCS3')\n",
    "        else:\n",
    "            ret.append(c)\n",
    "    return ret\n",
    "\n",
    "def final_result_logistic(data_columns):\n",
    "    num_pred = len(data_columns) - 1\n",
    "    predictor_columns = predictor_columns_for_logistic(data_columns)\n",
    "\n",
    "    if 0:\n",
    "        result = pd.concat([learning_logistic(encounter_list, predictor_columns, data_columns, num_pred, cv=5, ps='全体対象')])\n",
    "        \n",
    "    else:\n",
    "        result0 = learning_logistic(encounter_list, predictor_columns, data_columns, num_pred, cv=5, ps='全体対象')\n",
    "        result1 = learning_logistic(men_data, predictor_columns, data_columns, num_pred, cv=4, ps='男性のみ対象')\n",
    "        result2 = learning_logistic(women_data, predictor_columns, data_columns, num_pred, cv=4, ps='女性のみ対象')\n",
    "        result3 = learning_logistic(trauma_flag_1_data, predictor_columns, data_columns, num_pred, cv=3, ps='「外傷flag = 1」のみ対象')\n",
    "        result4 = learning_logistic(trauma_flag_2_data, predictor_columns, data_columns, num_pred, cv=4, ps='「外傷flag = 2」のみ対象')\n",
    "        result5 = learning_logistic(senior_data, predictor_columns, data_columns, num_pred, cv=4, ps='70歳以上のみ対象')\n",
    "        result6 = learning_logistic(young_data, predictor_columns, data_columns, num_pred, cv=4, ps='70歳未満のみ対象')\n",
    "\n",
    "        result = pd.concat([result0, result1, result2, result3, result4, result5, result6])\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_excellent_dataset(result_df):\n",
    "    d = {}\n",
    "    for i, t in result_df.iterrows():\n",
    "        key = t['Fitting Model'] + t['P.S.']\n",
    "        if key not in d:\n",
    "            d[key] = {tuple(t['Dataset']): t['AUC']}\n",
    "        else:\n",
    "            d[key].update({tuple(t['Dataset']): t['AUC']})\n",
    "\n",
    "    l = []\n",
    "    for k, v in d.items():\n",
    "        sorted_l = sorted(v.items(), key=lambda x: x[1], reverse=True)\n",
    "        l.append(sorted_l[0][0])\n",
    "\n",
    "    l = list(set(l))\n",
    "    l = [list(e) for e in l]\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. 年齢・性別・バイタルサインでの解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "Base_columns = ['Age', 'PHR_BasicInfo::Sex']\n",
    "Target_column = ['disposition']\n",
    "VS_columns = ['Cons_JCS','SBP', 'DBP', 'PR', 'BT']\n",
    "CC_columns = ['danger_level', 'danger_level_sonoo', 'danger_level_nino']\n",
    "Traums_columns = ['外傷flag']\n",
    "PMH_columns = [\n",
    "    'I10', 'E11', 'E14', 'I51', 'E78','I63', 'J45', 'I20', 'I48', 'F29', 'G30', 'K35', 'I50', 'I21', 'M48','K37',\n",
    "    'F03', 'K25', 'G40', 'C18', 'N18', 'H26', 'N19', 'F32', 'N40','K80', 'I49', 'E79', 'C16', 'R56'\n",
    "]\n",
    "\n",
    "result_list = []\n",
    "\n",
    "# array_mean_except_zero 内にてRuntimeWarning: Mean of empty slice が出るので抑制（予測変数が少なすぎるので予測性能の計算で警告が出る）\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    result_list.append(final_result(['Age'] + Target_column))\n",
    "    result_list.append(final_result(['PHR_BasicInfo::Sex'] + Target_column))\n",
    "    result_list.append(final_result(['Age', 'PHR_BasicInfo::Sex'] + Target_column))\n",
    "\n",
    "    target_columns_list = []\n",
    "    for i in range(len(VS_columns)):\n",
    "        target_columns_list.extend([Base_columns + list(e) + Target_column for e in list(itertools.combinations(VS_columns, i + 1))])\n",
    "\n",
    "    for data_columns in tqdm(target_columns_list):\n",
    "        result_list.append(final_result(data_columns))\n",
    "\n",
    "result_df = pd.concat(result_list)\n",
    "result_df.to_csv('results/basic_result.csv', encoding='utf_8_sig')\n",
    "\n",
    "excellence_basic_columns = extract_excellent_dataset(result_df)\n",
    "print(excellence_basic_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. 主訴と内因/外傷を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excellence_basic_columns = [['Age', 'PHR_BasicInfo::Sex', 'Cons_JCS', 'SBP', 'PR'], ['Age', 'PHR_BasicInfo::Sex', 'Cons_JCS', 'DBP', 'PR', 'BT'], ['Age', 'PHR_BasicInfo::Sex', 'Cons_JCS', 'SBP'], ['Age', 'PHR_BasicInfo::Sex', 'Cons_JCS', 'SBP', 'DBP', 'PR', 'BT'], ['Age', 'PHR_BasicInfo::Sex', 'Cons_JCS', 'SBP', 'BT'], ['Age', 'PHR_BasicInfo::Sex', 'Cons_JCS', 'BT'], ['Age', 'PHR_BasicInfo::Sex', 'Cons_JCS', 'DBP'], ['Age', 'PHR_BasicInfo::Sex', 'Cons_JCS', 'SBP', 'PR', 'BT']]\n",
    "\n",
    "target_columns_list = []\n",
    "for t in CC_columns + Traums_columns:\n",
    "    for c in excellence_basic_columns:\n",
    "        target_columns_list.append(c + [t] + Target_column)\n",
    "        \n",
    "result_list = []\n",
    "for data_columns in tqdm(target_columns_list):\n",
    "    result_list.append(final_result(data_columns))\n",
    "\n",
    "result_df = pd.concat(result_list)\n",
    "result_df.to_csv('results/cc_trauma_result.csv', encoding='utf_8_sig')\n",
    "\n",
    "excellence_cc_trauma_columns = extract_excellent_dataset(result_df)\n",
    "print(excellence_basic_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3. PMHを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PMHは既往歴を持っている人の割合が高い順に上位8個選ぶ（数値 * 100でパーセント値・一番上は44%）\n",
    "[\n",
    "    ('I10', 0.44717215661901805),\n",
    "    ('E11', 0.2973896830329397),\n",
    "    ('I51', 0.21814791796146674),\n",
    "    ('I63', 0.09571162212554382),\n",
    "    ('F29', 0.07986326911124922),\n",
    "    ('E78', 0.06650093225605966),\n",
    "    ('G30', 0.057178371659415785),\n",
    "    ('I48', 0.05593536357986327)\n",
    "]\n",
    "\n",
    "target_pmh_columns = ['I10', 'E11', 'I51', 'I63', 'F29', 'E78', 'G30', 'I48']\n",
    "\n",
    "target_columns_list = []\n",
    "for cc_trauma_columns in excellence_cc_trauma_columns:\n",
    "    for i in range(8):\n",
    "        for columns in [list(e) for e in list(itertools.combinations(target_pmh_columns, i + 1))]:\n",
    "            target_columns_list.append(cc_trauma_columns + columns + Target_column)\n",
    "            \n",
    "result_list = []\n",
    "for data_columns in tqdm(target_columns_list):\n",
    "    result_list.append(final_result(data_columns))\n",
    "\n",
    "result_df = pd.concat(result_list)\n",
    "result_df.to_csv('results/pmh_result.csv', encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.groupby(['Number of predictors', 'Fitting Model', 'P.S.'])['AUC'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 年齢・JCSをカテゴリ変数にしてロジスティックで同様に"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "Base_columns = ['DiscreteAge', 'PHR_BasicInfo::Sex']\n",
    "Target_column = ['disposition']\n",
    "VS_columns = ['JCS','SBP', 'DBP', 'PR', 'BT']\n",
    "CC_columns = ['danger_level', 'danger_level_sonoo', 'danger_level_nino']\n",
    "Traums_columns = ['外傷flag']\n",
    "PMH_columns = [\n",
    "    'I10', 'E11', 'E14', 'I51', 'E78','I63', 'J45', 'I20', 'I48', 'F29', 'G30', 'K35', 'I50', 'I21', 'M48','K37',\n",
    "    'F03', 'K25', 'G40', 'C18', 'N18', 'H26', 'N19', 'F32', 'N40','K80', 'I49', 'E79', 'C16', 'R56'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1. Logistic 年齢・性別・バイタルサインでの解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "\n",
    "# array_mean_except_zero 内にてRuntimeWarning: Mean of empty slice が出るので抑制（予測変数が少なすぎるので予測性能の計算で警告が出る）\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    result_list.append(final_result_logistic(['DiscreteAge'] + Target_column))\n",
    "    result_list.append(final_result_logistic(['PHR_BasicInfo::Sex'] + Target_column))\n",
    "    result_list.append(final_result_logistic(['DiscreteAge', 'PHR_BasicInfo::Sex'] + Target_column))    \n",
    "\n",
    "    target_columns_list = []\n",
    "    for i in range(len(VS_columns)):\n",
    "        target_columns_list.extend([Base_columns + list(e) + Target_column for e in list(itertools.combinations(VS_columns, i + 1))])\n",
    "\n",
    "    for data_columns in tqdm(target_columns_list):\n",
    "        result_list.append(final_result_logistic(data_columns))\n",
    "    \n",
    "    \n",
    "result_df = pd.concat(result_list)\n",
    "result_df.to_csv('results/basic_result_logistic.csv', encoding='utf_8_sig')\n",
    "\n",
    "excellence_basic_columns = extract_excellent_dataset(result_df)\n",
    "print(excellence_basic_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2. Logistic 主訴と内因/外傷を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excellence_basic_columns = [['DiscreteAge', 'PHR_BasicInfo::Sex', 'JCS', 'SBP', 'PR', 'BT'], ['DiscreteAge', 'PHR_BasicInfo::Sex', 'JCS', 'BT'], ['DiscreteAge', 'PHR_BasicInfo::Sex', 'JCS', 'SBP', 'DBP', 'PR', 'BT'], ['DiscreteAge', 'PHR_BasicInfo::Sex', 'JCS', 'DBP', 'PR', 'BT'], ['DiscreteAge', 'PHR_BasicInfo::Sex', 'JCS', 'DBP', 'BT']]\n",
    "\n",
    "target_columns_list = []\n",
    "for t in CC_columns + Traums_columns:\n",
    "    for c in excellence_basic_columns:\n",
    "        target_columns_list.append(c + [t] + Target_column)\n",
    "        \n",
    "result_list = []\n",
    "for data_columns in tqdm(target_columns_list):\n",
    "    result_list.append(final_result_logistic(data_columns))\n",
    "\n",
    "result_df = pd.concat(result_list)\n",
    "result_df.to_csv('results/cc_trauma_result_logistic.csv', encoding='utf_8_sig')\n",
    "\n",
    "excellence_cc_trauma_columns = extract_excellent_dataset(result_df)\n",
    "print(excellence_cc_trauma_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-3. Logistic PMHを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excellence_cc_trauma_columns = [['DiscreteAge', 'PHR_BasicInfo::Sex', 'JCS', 'SBP', 'PR', 'BT', 'danger_level_sonoo'], ['DiscreteAge', 'PHR_BasicInfo::Sex', 'JCS', 'DBP', 'PR', 'BT', 'danger_level_sonoo'], ['DiscreteAge', 'PHR_BasicInfo::Sex', 'JCS', 'BT', 'danger_level_sonoo'], ['DiscreteAge', 'PHR_BasicInfo::Sex', 'JCS', 'SBP', 'DBP', 'PR', 'BT', 'danger_level_sonoo']]\n",
    "\n",
    "# PMHは既往歴を持っている人の割合が高い順に上位8個選ぶ（数値 * 100でパーセント値・一番上は44%）\n",
    "[\n",
    "    ('I10', 0.44717215661901805),\n",
    "    ('E11', 0.2973896830329397),\n",
    "    ('I51', 0.21814791796146674),\n",
    "    ('I63', 0.09571162212554382),\n",
    "    ('F29', 0.07986326911124922),\n",
    "    ('E78', 0.06650093225605966),\n",
    "    ('G30', 0.057178371659415785),\n",
    "    ('I48', 0.05593536357986327)\n",
    "]\n",
    "\n",
    "target_pmh_columns = ['I10', 'E11', 'I51', 'I63', 'F29', 'E78', 'G30', 'I48']\n",
    "\n",
    "target_columns_list = []\n",
    "for cc_trauma_columns in excellence_cc_trauma_columns:\n",
    "    for i in range(8):\n",
    "        for columns in [list(e) for e in list(itertools.combinations(target_pmh_columns, i + 1))]:\n",
    "            target_columns_list.append(cc_trauma_columns + columns + Target_column)\n",
    "            \n",
    "result_list = []\n",
    "for data_columns in tqdm(target_columns_list):\n",
    "    result_list.append(final_result_logistic(data_columns))\n",
    "\n",
    "result_df = pd.concat(result_list)\n",
    "result_df.to_csv('results/pmh_result_logistic.csv', encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excellence_basic_columns = extract_excellent_dataset(result_df)\n",
    "print(excellence_basic_columns)\n",
    "\n",
    "print(result_df.groupby(['Number of predictors', 'Fitting Model', 'P.S.'])['AUC'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
